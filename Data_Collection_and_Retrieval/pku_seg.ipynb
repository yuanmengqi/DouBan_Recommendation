{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkuseg\n",
    "import pandas as pd\n",
    "col=7\n",
    "seg = pkuseg.pkuseg()           # 以默认配置加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_(filepath, docs):\n",
    "    lenth=len(docs)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().split('\\n')\n",
    "        # 删去docs中的停用词\n",
    "        for i in range(0, lenth):\n",
    "            #print(docs[i])\n",
    "            for j in range(0, col):\n",
    "                try:\n",
    "                    k=0\n",
    "                    while (k < len(docs[i][j])):  # for循环遍历分词后的每个词语\n",
    "                        # 若字符在停用表中\n",
    "                        if docs[i][j][k] in stopwords:\n",
    "                            docs[i][j].remove(docs[i][j][k])\n",
    "                        else:\n",
    "                            k+=1\n",
    "                except:\n",
    "                    continue\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把word2doc保存到文件中\n",
    "import pickle\n",
    "\n",
    "def save_dict(dictionary, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(dictionary, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 近义词合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建近义词表\n",
    "def synwords_(syn_path):\n",
    "    synwords = []\n",
    "    with open(syn_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            split = line.strip().split(' ')\n",
    "            number = split[0]\n",
    "            templist = split[1:]\n",
    "            if number[-1] == '=':\n",
    "                synwords.append(templist)\n",
    "    save_dict(synwords, './data_dict/synwords.pickle')\n",
    "    return synwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findsyn(word, synwords):\n",
    "    if len(word) == 1:\n",
    "        return -1\n",
    "    for line in synwords:\n",
    "        if word in line:\n",
    "            return line\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synwords = synwords_('./chinese_dictionary-master/dict_synonym.txt')\n",
    "findsyn('犯罪', synwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn_merge(docs,synwords):\n",
    "    #将近义词替换为同义词列表第一个词\n",
    "    #遍历分词、去停用词后的结果\n",
    "    lenth=len(docs)\n",
    "    for i in range(0, lenth):\n",
    "        for j in range(0, col):\n",
    "            for k in range(0, len(docs[i][j])):\n",
    "                if findsyn(docs[i][j][k], synwords) != -1:\n",
    "                    docs[i][j][k] = findsyn(docs[i][j][k], synwords)[0]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import lazy_pinyin\n",
    "# 建立倒排索引表\n",
    "\n",
    "def build(docs,syn=0):\n",
    "    #syn=0不处理近义词\n",
    "    #syn=1处理近义词\n",
    "    if syn==1:\n",
    "        docs=syn_merge(docs,synwords)\n",
    "        #print(\"syn_merge_doc=\")\n",
    "        #print(docs)\n",
    "    word2doc = {}\n",
    "    for i in range(0, len(docs)):\n",
    "        temp_freq = {}  # 记录当前文档中词频\n",
    "        for j in range(0, col):\n",
    "            for word in docs[i][j]:\n",
    "                if word in temp_freq:\n",
    "                    temp_freq[word] += 1\n",
    "                else:\n",
    "                    temp_freq[word] = 1\n",
    "        # print(temp_freq)\n",
    "        for word in temp_freq:\n",
    "            if word in word2doc:\n",
    "                word2doc[word].append((i, temp_freq[word]))\n",
    "            else:\n",
    "                word2doc[word] = [(i, temp_freq[word])]\n",
    "    print(type(word2doc))\n",
    "    word2doc = sorted(word2doc.items(), key=lambda x: lazy_pinyin(x[0]))\n",
    "    # list转成dict\n",
    "    word2doc = dict(word2doc)\n",
    "    print(type(word2doc))\n",
    "    return word2doc\n",
    "# print(word2doc)\n",
    "# word2doc按词的字母序排序\n",
    "# word2doc = sorted(word2doc.items(),key=lambda x:x[0])\n",
    "# print(word2doc)\n",
    "# word2doc按汉语拼音首字母排序\n",
    "# word2doc = sorted(word2doc.items(),key=lambda x:lazy_pinyin(x[0]))\n",
    "# print(word2doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 电影\n",
    "#### 读取电影信息并分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取movie/MovieDouban_0.csv文件第一列\n",
    "df = pd.read_csv('./movie/MovieDouban_p0.csv',\n",
    "                 encoding='utf-8', usecols=[4, 12, 13, 20, 21,7,9], header=0)\n",
    "for num in range(1, 12):\n",
    "    df1 = pd.read_csv('./movie/MovieDouban_p{}.csv'.format(num),\n",
    "                      encoding='utf-8', usecols=[4, 12, 13, 20, 21,7,9], header=0)\n",
    "    df = pd.concat([df, df1], axis=0, ignore_index=True)\n",
    "# movie = df.to_string()\n",
    "# print(movie)\n",
    "# movie = seg.cut(movie)\n",
    "# lenth=pd.value_counts(df).count()\n",
    "\n",
    "movie_raw = df.values.tolist()\n",
    "save_dict(movie_raw, './data_dict/movie_raw.pickle')\n",
    "# movie_raw不随movie变化\n",
    "movie = df.values.tolist()\n",
    "lenth = len(movie)\n",
    "print(lenth)\n",
    "# print(movie[0][0])\n",
    "# print(type(movie[0][0]))#str\n",
    "for i in range(0, lenth):\n",
    "    for j in range(0, col):\n",
    "        # 大写转小写\n",
    "        try:\n",
    "            #去除·\n",
    "            movie[i][j]=movie[i][j].replace('·','')\n",
    "            movie[i][j] = movie[i][j].lower()\n",
    "            movie[i][j] = seg.cut(movie[i][j])\n",
    "        except:\n",
    "            movie[i][j] = ''\n",
    "        print(movie[i][j])\n",
    "print(type(movie))\n",
    "#print(movie)\n",
    "#print(movie_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立电影倒排表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_s1 = stopwords_('./stopwords-master/hit_stopwords.txt', movie)\n",
    "movie_s2 = stopwords_(\n",
    "    './stopwords-master/baidu_stopwords.txt', movie_s1)\n",
    "#print(movie_s2)\n",
    "#print(movie_s2[97])\n",
    "word2doc_movie = build(movie_s2,syn=1)\n",
    "#print(word2doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 书籍\n",
    "#### 读取书籍信息并分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取book/bookDouban_0.csv文件第一列\n",
    "df = pd.read_csv('./book/BookDouban_p0.csv',\n",
    "                 encoding='utf-8', usecols=[1, 5, 6, 14, 16,8,9], header=0)\n",
    "for num in range(1, 12):\n",
    "    df1 = pd.read_csv('./Book/bookDouban_p{}.csv'.format(num),\n",
    "                      encoding='utf-8', usecols=[1, 5, 6, 14, 16,8,9], header=0)\n",
    "    df = pd.concat([df, df1], axis=0, ignore_index=True)\n",
    "# book = df.to_string()\n",
    "# print(book)\n",
    "# book = seg.cut(book)\n",
    "# lenth=pd.value_counts(df).count()\n",
    "\n",
    "book_raw = df.values.tolist()\n",
    "save_dict(book_raw, './data_dict/book_raw.pickle')\n",
    "# book_raw不随book变化\n",
    "book = df.values.tolist()\n",
    "lenth = len(book)\n",
    "print(lenth)\n",
    "# print(book[0][0])\n",
    "# print(type(book[0][0]))#str\n",
    "for i in range(0, lenth):\n",
    "    for j in range(0, col):\n",
    "        # 大写转小写\n",
    "        try:\n",
    "            #去除·\n",
    "            book[i][j]=book[i][j].replace('·','')\n",
    "            book[i][j] = book[i][j].lower()\n",
    "            book[i][j] = seg.cut(book[i][j])\n",
    "        except:\n",
    "            book[i][j] = ''\n",
    "print(type(book))\n",
    "# print(book)\n",
    "#print(book_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立书籍倒排表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_s1 = stopwords_('./stopwords-master/hit_stopwords.txt', book)\n",
    "book_s2 = stopwords_(\n",
    "    './stopwords-master/baidu_stopwords.txt', book_s1)\n",
    "#print(book_s2)\n",
    "#print(book_s2[97])\n",
    "word2doc_book = build(book_s2,syn=1)\n",
    "#print(word2doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 倒排表保存到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存字典到文件\n",
    "save_dict(word2doc_movie, './data_dict/word2doc_movie.pickle')\n",
    "save_dict(word2doc_book, './data_dict/word2doc_book.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "def insert(root, word):\n",
    "    curr = root\n",
    "    for char in word:\n",
    "        if char not in curr.children:\n",
    "            curr.children[char] = TrieNode()\n",
    "        curr = curr.children[char]\n",
    "    curr.is_end_of_word = True\n",
    "\n",
    "def compress(strings):\n",
    "    root = TrieNode()\n",
    "    for string in strings:\n",
    "        insert(root, string)\n",
    "    \n",
    "    compressed = []\n",
    "    for string in strings:\n",
    "        curr = root\n",
    "        prefix = ''\n",
    "        for char in string:\n",
    "            if char not in curr.children or len(curr.children) > 1:\n",
    "                break\n",
    "            curr = curr.children[char]\n",
    "            prefix += char\n",
    "        suffix = string[len(prefix):]\n",
    "        compressed.append((prefix, suffix))\n",
    "    \n",
    "    return compressed\n",
    "\n",
    "def decompress(compressed):\n",
    "    root = TrieNode()\n",
    "    for prefix, suffix in compressed:\n",
    "        insert(root, prefix + suffix)\n",
    "    \n",
    "    decompressed = []\n",
    "    for prefix, suffix in compressed:\n",
    "        curr = root\n",
    "        string = prefix\n",
    "        for char in suffix:\n",
    "            if char not in curr.children:\n",
    "                break\n",
    "            curr = curr.children[char]\n",
    "            string += char\n",
    "        decompressed.append(string)\n",
    "    \n",
    "    return decompressed\n",
    "\n",
    "# 示例用法\n",
    "strings = ['apple', 'app', 'application', 'banana']\n",
    "compressed = compress(strings)\n",
    "print('Compressed:', compressed)\n",
    "\n",
    "decompressed = decompress(compressed)\n",
    "print('Decompressed:', decompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按块压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#二分查找\n",
    "def binary_search(list, target):\n",
    "    low = 0\n",
    "    high = len(list) - 1\n",
    "\n",
    "    while low <= high:\n",
    "        mid = int((low + high) / 2)\n",
    "        if list[mid][0] == target:\n",
    "            return list[mid][1]\n",
    "        elif list[mid][0] > target:\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            low = mid + 1\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def zipped_dict(dictpath):\n",
    "    key_zipped=\"\"\n",
    "    list_zipped={}\n",
    "    ptr=0\n",
    "    with open(dictpath, 'rb') as f:\n",
    "        dict_movie=pickle.load(f)\n",
    "        for key in dict_movie:\n",
    "            #if(key=='活'):\n",
    "            #    print(key,ptr)\n",
    "            #print(ptr)\n",
    "            key_zipped+=key\n",
    "            list_zipped[ptr]=dict_movie[key]\n",
    "            ptr=ptr+len(key)\n",
    "    print(key_zipped)\n",
    "    print(list_zipped)\n",
    "    #保存到文件\n",
    "    \n",
    "    save_dict(key_zipped,'./data_dict/key_zipped_{}.pickle'.format(dictpath[21:-7]))\n",
    "    save_dict(list_zipped,'./data_dict/list_zipped_{}.pickle'.format(dictpath[21:-7]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "def zipped_dict(dictpath):\n",
    "    key_zipped=\"\"\n",
    "    list_zipped=[]\n",
    "    ptr=0\n",
    "    with open(dictpath, 'rb') as f:\n",
    "        dict_movie=pickle.load(f)\n",
    "        for key in dict_movie:\n",
    "            #if(key=='活'):\n",
    "            #    print(key,ptr)\n",
    "            #print(ptr)\n",
    "            key_zipped+=key\n",
    "            no.append(ptr)\n",
    "            list_zipped.append(dict_movie[key])\n",
    "            ptr=ptr+len(key)+len(str(no))\n",
    "    print(key_zipped)\n",
    "    print(list_zipped)\n",
    "    #保存到文件\n",
    "    \n",
    "    save_dict(key_zipped,'./data_dict/key_zipped_{}.pickle'.format(dictpath[21:-7]))\n",
    "    save_dict(list_zipped,'./data_dict/list_zipped_{}.pickle'.format(dictpath[21:-7]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#在压缩字典中查找\n",
    "def find_zipped(key_zipped,list_zipped,word):\n",
    "    if word in key_zipped:\n",
    "        index=key_zipped.find(word)\n",
    "        print(word,index)\n",
    "        #二分检索list_zipped第一维为index的元组\n",
    "        return binary_search(list_zipped,index)\n",
    "    else:\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在压缩字典中查找\n",
    "def find_zipped(key_zipped,list_zipped,word):\n",
    "    index=0\n",
    "    if word in key_zipped:\n",
    "        while(index<len(key_zipped)):\n",
    "            index=key_zipped[index+1:].find(word)+index+1#第一次出现的位置\n",
    "            print(word,index)\n",
    "            if index in list_zipped:\n",
    "                #print('--------------------')\n",
    "                return list_zipped[index]\n",
    "    else:\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_dict('./data_dict/word2doc_movie.pickle')\n",
    "zipped_dict('./data_dict/word2doc_book.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_plk(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_zipped=load_plk('./data_dict/key_zipped_movie.pickle')\n",
    "list_zipped=load_plk('./data_dict/list_zipped_movie.pickle')\n",
    "print(list_zipped)\n",
    "dict_movie=load_plk('./data_dict/word2doc_movie.pickle')\n",
    "print(find_zipped(key_zipped,list_zipped,'活'))\n",
    "#print(type(key_zipped[9574:]))\n",
    "#print(key_zipped[9574:].find(\"活\"))\n",
    "#print(key_zipped[9573:9575])\n",
    "print(dict_movie[\"活\"])#第8个，索引24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygtrie import Trie\n",
    "\n",
    "# 构建前缀树\n",
    "trie = Trie()\n",
    "trie['我懂'] = 1\n",
    "trie['我'] = 2\n",
    "trie['我爱你'] = 2\n",
    "trie['orange'] = 3\n",
    "\n",
    "# 查询前缀树\n",
    "print(trie.get('我'))  # 输出: 1\n",
    "print(trie.keys(prefix='我'))  # 输出: ['banana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pygtrie import Trie\n",
    "\n",
    "# 构建前缀树\n",
    "def trie_zip(dictpath):\n",
    "    with open(dictpath, 'rb') as f:\n",
    "        dict_movie=pickle.load(f)\n",
    "    trie = Trie()\n",
    "    for word in dict_movie:\n",
    "        trie[word] = dict_movie[word]\n",
    "    save_dict(trie,'./data_dict/gtrie_{}.pickle'.format(dictpath[21:-7]))\n",
    "\n",
    "trie_zip('./data_dict/word2doc_movie.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrie import StringTrie\n",
    "\n",
    "# 构建前缀树\n",
    "def trie_zip(dictpath):\n",
    "    with open(dictpath, 'rb') as f:\n",
    "        dict_movie=pickle.load(f)\n",
    "    trie = StringTrie()\n",
    "    for word in dict_movie:\n",
    "        trie[word] = dict_movie[word]\n",
    "    save_dict(trie,'./data_dict/trie_{}.pickle'.format(dictpath[21:-7]))\n",
    "\n",
    "trie_zip('./data_dict/word2doc_movie.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
